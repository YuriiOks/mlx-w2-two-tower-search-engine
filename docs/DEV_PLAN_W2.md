# Week 2 Dev Plan: Two-Tower Search Engine (MS MARCO) üóºüóº

**Team:** Perceptron Party (Yurii, Pyry, Dimitris, Dimitar)
**Goal:** Build and train a Two-Tower model using RNNs and Triplet Loss for document retrieval on the MS MARCO dataset, leveraging pre-trained embeddings from Week 1.

*(Collaboration: Pair programming is encouraged! Assignees are suggestions, feel free to swap or work together.)*

| Step  | Task Area                      | Specific Task                                       | Description & Details                                                                                                                                                                                                | Focus Suggestion | Git Branch Suggestion      | Status      |
| :---- | :----------------------------- | :-------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------- | :------------------------- | :---------- |
| **P2.0** | **Setup & Config** ‚öôÔ∏è       | **Initialize Project & Verify Config**              | Run `setup_project.py` ‚úÖ. Initialize Git repo ‚úÖ. Set up venv & install `requirements.txt` ‚úÖ. **Crucially: Update `config.yaml` with correct paths to W1 vocab & embeddings** ‚úÖ. Review all config params.                 | Team (All)       | `main`                     | ‚úÖ Done      |
| **P2.1** | **Research & Data Prep** üìö | **Understand MS MARCO & Triplet Strategy**          | Research MS MARCO v1.1 format (HF `datasets` recommended?). Understand `query`, `passages`, `is_selected`. Decide on a **negative sampling strategy** for triplets (e.g., random non-positives from batch? Hard negatives?). | Dimitris / Pyry  | `feature/data-prep`      | ‚è≥ To Do     |
| P2.2  | **Data Prep** üßπ               | **Load & Tokenize Data**                            | Implement loading MS MARCO (e.g., using `datasets.load_dataset("ms_marco", "v1.1")`). Implement tokenization for queries/passages **using the loaded W1 Vocabulary**. Handle padding/truncation. (`src/two_tower/dataset.py`) | Dimitris / Pyry  | `feature/data-prep`      | ‚è≥ To Do     |
| P2.3  | **Data Prep** üßπ               | **Implement Triplet Generation & Dataset/Collate**  | Based on P2.1 strategy, implement logic to generate `(query_ids, pos_doc_ids, neg_doc_ids)` triplets. Implement `TripletDataset` and `collate_triplets` function (handles padding). (`src/two_tower/dataset.py`)           | Dimitris / Pyry  | `feature/data-prep`      | ‚è≥ To Do     |
| P2.4  | **Model Implementation** üèóÔ∏è     | **Load Embeddings & Define RNN Towers**             | Implement logic in `TwoTowerModel` (`src/two_tower/model.py`) to load pre-trained W1 embeddings. Implement `QueryEncoder` and `DocumentEncoder` using an RNN type (`config: two_tower.model_type` - start with GRU or LSTM). | Yurii / Dimitar  | `feature/model-arch`     | ‚è≥ To Do     |
| P2.5  | **Model Implementation** üèóÔ∏è     | **Implement `TwoTowerModel` Forward Pass**          | Complete the `forward` method in `TwoTowerModel` to take query/pos/neg IDs, pass them through embeddings and respective encoders, return the final query/pos/neg embeddings. Handle shared vs separate doc encoder.           | Yurii / Dimitar  | `feature/model-arch`     | ‚è≥ To Do     |
| P2.6  | **Training Implementation** üîÑ | **Implement Triplet Loss Training Epoch**         | Implement `train_epoch_two_tower` in `src/two_tower/trainer.py`. Calculate pairwise distances (cosine recommended), apply triplet margin loss (`F.relu(dist_pos - dist_neg + margin)`), perform backprop.                    | Pyry / Yurii     | `feature/training-loop`  | ‚è≥ To Do     |
| P2.7  | **Training Implementation** üîÑ | **Implement `train_two_tower_model` Orchestrator**  | Implement the main training orchestrator function in `src/two_tower/trainer.py`. Handle epochs, call `train_epoch`, log metrics to W&B, save final model.                                                              | Pyry / Yurii     | `feature/training-loop`  | ‚è≥ To Do     |
| P2.8  | **Main Script** ‚ñ∂Ô∏è              | **Update `scripts/train_two_tower.py`**             | Integrate all components: Load config/args, setup W&B, load vocab/embeddings, prepare data (call dataset functions), initialize model/optimizer, call trainer, log artifacts.                                              | Team (All)       | `feature/training-script`| ‚è≥ To Do     |
| P2.9  | **Initial Training Run** üî•    | **Run Training & Debug**                            | Execute `scripts/train_two_tower.py` on a subset of data or for few epochs. Debug any issues in data pipeline, model, loss, or training loop. Monitor on W&B.                                                     | Team (All)       | `feature/initial-training`| ‚è≥ To Do     |
| **P2.10**| **Research & Future** üí°    | **Research Inference & Vector DBs**                 | Investigate efficient similarity search methods (e.g., FAISS, Annoy). Research how vector databases like **ChromaDB** or Weaviate work and how pre-cached document embeddings could be stored/queried for inference.         | Dimitar / James? | `docs/research`          | ‚è≥ To Do     |
| P2.11 | **(Stretch) Evaluation** üìä     | **Implement Basic Evaluation**                      | Implement `scripts/evaluate_two_tower.py` (or notebook). Load trained model, encode eval queries & docs, calculate Recall@k or MRR based on ground truth relevance.                                                  | Optional         | `feature/evaluation`     | ‚è≥ To Do     |
| P2.12 | **(Stretch) RNN Variants** üöÄ   | **Experiment with GRU/LSTM/BiRNN**                  | If time permits, modify the `rnn_type` and `bidirectional` parameters in `config.yaml` and retrain to compare performance of different RNN encoders.                                                            | Optional         | `feature/rnn-variants`   | ‚è≥ To Do     |

