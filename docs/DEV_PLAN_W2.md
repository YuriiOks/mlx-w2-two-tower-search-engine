# Week 2 Dev Plan: Two-Tower Search Engine (MS MARCO) üóºüóº

**Team:** Perceptron Party (Yurii, Pyry, Dimitris, Dimitar)
**Goal:** Implement and train two types of Two-Tower models (RNN-based and Simple Average Pooling) using Triplet Loss on MS MARCO, leveraging pre-trained embeddings from Week 1.

*(Collaboration: Please sync frequently, especially between data prep and model implementation teams! Pair programming encouraged.)*

---
**Day 1: Setup, Data Pipeline & Model Structures**
---

| Time Block  | Task Area             | Specific Task                                       | Description & Details                                                                                                                                                               | Assignee Suggestion     | Git Branch Suggestion       | Status      |
| :---------- | :-------------------- | :-------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------- | :-------------------------- | :---------- |
| **Morning** | **Setup & Sync** ‚öôÔ∏è | **Environment & Config Verification**               | All: Clone repo, setup/activate `.venv`, `pip install -r requirements.txt`, `wandb login`. **CRITICAL: Update local `config.yaml` paths** for W1 vocab/embeddings. Review plan.          | Team (All)              | `main` (then branch off)  | ‚è≥ To Do     |
| Morning     | **Data Pipeline** üìö  | **MS MARCO Research & Load Strategy**               | Research MS MARCO v1.1 structure (Hugging Face `datasets` highly recommended!). Decide initial strategy for loading data and **triplet negative sampling** (e.g., random in-batch).   | Pyry / Dimitris         | `feature/data-pipeline`   | ‚è≥ To Do     |
| Morning     | **RNN Model** üèóÔ∏è      | **Define RNN Encoder Classes**                      | Define `QueryEncoder` and `DocumentEncoder` classes using RNN/GRU/LSTM based on `config.yaml`. Implement basic `forward` pass logic (handle hidden states). (`src/two_tower/model_rnn.py`) | Yurii                   | `feature/rnn-model`       | ‚è≥ To Do     |
| Morning     | **Simple Model** üèóÔ∏è   | **Define Simple Avg Pooling "Encoder"**             | Create a simple function or lightweight class that takes token embeddings and returns their average (e.g., mean pooling over sequence length, ignore padding). (`src/two_tower/model_simple.py`) | Dimitar                 | `feature/simple-model`    | ‚è≥ To Do     |
| **Afternoon** | **Data Pipeline** üßπ  | **Implement Data Loading & Tokenization**           | Implement function(s) to load MS MARCO data (queries, passages). Implement tokenization **using the loaded W1 Vocabulary**. Handle padding/truncation. (`src/two_tower/dataset.py`)         | Pyry / Dimitris         | `feature/data-pipeline`   | ‚è≥ To Do     |
| Afternoon | **RNN Model** üèóÔ∏è      | **Implement RNN Two-Tower Structure**               | Create `TwoTowerModelRNN` class. Integrate `nn.Embedding` (loading W1 weights, handling freezing), `QueryEncoder`, `DocumentEncoder`. Implement main `forward` pass returning triplet embeddings. | Yurii                   | `feature/rnn-model`       | ‚è≥ To Do     |
| Afternoon | **Simple Model** üèóÔ∏è   | **Implement Simple Two-Tower Structure**            | Create `TwoTowerModelSimple` class. Integrate `nn.Embedding` (loading W1 weights, handling freezing) and the average pooling logic for both query and document towers. Implement main `forward` pass. | Dimitar                 | `feature/simple-model`    | ‚è≥ To Do     |
| End of Day  | **Sync & Commit** üîÑ  | **Push Branches & Brief Sync-up**                   | All: Commit & push progress on respective feature branches. Quick chat on blockers/progress.                                                                                          | Team (All)              | *(Feature Branches)*      | ‚è≥ To Do     |

---
**Day 2: Triplet Generation, Training Logic & Initial Runs**
---

| Time Block  | Task Area              | Specific Task                                       | Description & Details                                                                                                                                                                | Assignee Suggestion     | Git Branch Suggestion       | Status      |
| :---------- | :--------------------- | :-------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------- | :-------------------------- | :---------- |
| **Morning** | **Data Pipeline** üßπ   | **Implement Triplet Generation & Dataset/Collate**  | Implement the chosen negative sampling strategy to create `(q_ids, pos_ids, neg_ids)` triplets. Finalize `TripletDataset` and `collate_triplets` function (handles padding correctly).     | Pyry / Dimitris         | `feature/data-pipeline`   | ‚è≥ To Do     |
| Morning     | **Training Logic** üîÑ  | **Implement Triplet Loss & Trainer**                | Implement `train_epoch_two_tower` using Triplet Loss logic (`F.relu(dist_pos - dist_neg + margin)`) and cosine distance (or chosen metric). Finalize `train_two_tower_model` orchestrator. (`src/two_tower/trainer.py`) | Yurii / Dimitar         | `feature/training-loop`   | ‚è≥ To Do     |
| Morning     | **Scripting** ‚ñ∂Ô∏è       | **Refine `train_two_tower.py` (Part 1)**            | Modify script to import correct modules. Add logic to select model (`TwoTowerModelRNN` vs `TwoTowerModelSimple`) based on `config.yaml`/args. Ensure vocab/embeddings loaded correctly.        | Team (Sync Needed)      | `feature/training-script` | ‚è≥ To Do     |
| **Afternoon** | **Integration** üß©     | **Combine Data, Models & Trainer**                  | Ensure the data pipeline output (`DataLoader` with `collate_fn`) works with both model types and the trainer function. Debug data shapes and types.                                   | Team (All - Pair/Group) | `feature/integration`     | ‚è≥ To Do     |
| Afternoon | **Initial Tests** üî•   | **Run Small Test Trainings**                        | Modify `train_two_tower.py` (or use args) for small data subset/epochs. Run for *both* RNN and Simple models. Debug end-to-end flow & check W&B logging. Ensure loss decreases initially. | Team (All - Pair/Group) | `feature/integration`     | ‚è≥ To Do     |
| Afternoon | **Research** üí°        | **Vector DB / Inference Research**                  | Start reading about ChromaDB/FAISS. How are embeddings added? How is similarity search performed? What are the requirements for deployment?                                           | Pyry / Dimitris / Dimitar | `docs/research-notes.md`  | ‚è≥ To Do     |
| End of Day  | **Review & Plan Next** | **Review Progress, Discuss Results, Plan Phase 2b** | Assess results of initial runs. Check W&B. Plan next steps (full training runs, evaluation implementation, inference pipeline).                                                     | Team (All)              | `main` (via PRs)          | ‚è≥ To Do     |
