# MS MARCO Search Engine - Project Structure

This document outlines the detailed directory structure and the purpose of all key files for the Week 2 Two-Tower Search Engine project by **Team Perceptron Party**.

## Directory Tree ğŸŒ³

```
ğŸ“ mlx-w2-two-tower-search-engine/
â”œâ”€â”€ ğŸ“ app/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ __init__.py
â”œâ”€â”€ â””â”€â”€ ğŸ“„ app.py
â”œâ”€â”€ ğŸ“ data/
â”œâ”€â”€ â””â”€â”€ ğŸ“ msmarco/
â”œâ”€â”€ ğŸ“ data_triplets/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ test_triplets.csv
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ train_triplets.csv
â”œâ”€â”€ â””â”€â”€ ğŸ“„ val_triplets.csv
â”œâ”€â”€ ğŸ“ docs/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ DEV_PLAN_W2.md
â”œâ”€â”€ â””â”€â”€ ğŸ“„ STRUCTURE.MD
â”œâ”€â”€ ğŸ“ logs/
â”œâ”€â”€ ğŸ“ models/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“ two_tower/
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“ TwoTower_E3_LR1e-05_BS128/
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ training_loss.png
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ training_losses.json
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ ğŸ“„ two_tower_final.pth
â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ ğŸ“ TwoTower_E5_LR0.0001_BS128/
â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ ğŸ“„ training_loss.png
â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ ğŸ“„ training_losses.json
â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ â””â”€â”€ ğŸ“„ two_tower_final.pth
â”œâ”€â”€ â””â”€â”€ ğŸ“ word2vec/
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ ğŸ“ CBOW_D128_W5_NWAll_MF5_E15_LR0.001_BS512/
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ cbow_training_loss.png
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ cbow_training_losses.json
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ â””â”€â”€ ğŸ“„ model_state.pth
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ ğŸ“ SkipGram_D128_W5_NWAll_MF5_E3_LR0.001_BS512/
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ model_state.pth
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ training_loss.png
â”œâ”€â”€ â””â”€â”€ â”œâ”€â”€ â””â”€â”€ ğŸ“„ training_losses.json
â”œâ”€â”€ â””â”€â”€ â””â”€â”€ ğŸ“„ text8_vocab.json
â”œâ”€â”€ ğŸ“ notebooks/
â”œâ”€â”€ ğŸ“ scripts/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ evaluate_two_tower.py
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ generate_project_doc.py
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ inspect_hf_dataset.py
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ train_two_tower.py
â”œâ”€â”€ â””â”€â”€ ğŸ“„ train_word2vec.py
â”œâ”€â”€ ğŸ“ src/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“ two_tower/
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ __init__.py
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ dataset.py
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ model.py
â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ ğŸ“„ trainer.py
â”œâ”€â”€ â”œâ”€â”€ ğŸ“ word2vec/
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ __init__.py
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ dataset.py
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ model.py
â”œâ”€â”€ â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ trainer.py
â”œâ”€â”€ â”œâ”€â”€ â””â”€â”€ ğŸ“„ vocabulary.py
â”œâ”€â”€ â””â”€â”€ ğŸ“„ __init__.py
â”œâ”€â”€ ğŸ“ utils/
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ __init__.py
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ device_setup.py
â”œâ”€â”€ â”œâ”€â”€ ğŸ“„ logging.py
â”œâ”€â”€ â””â”€â”€ ğŸ“„ run_utils.py
â”œâ”€â”€ ğŸ“„ .flake8
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ Dockerfile
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ config.yaml
â”œâ”€â”€ ğŸ“„ marco_entry.txt
â”œâ”€â”€ ğŸ“„ ms_marco_entry.txt
â”œâ”€â”€ ğŸ“„ requirements.txt
â””â”€â”€ ğŸ“„ setup_project.py
```

## Detailed Directory & File Descriptions ğŸ“œ

*   **Root Directory (`mlx-w2-two-tower-search-engine/`)**: Contains configuration, main scripts, documentation, and subdirectories for source code, data, models, etc.
    *   ğŸ“„ **`.flake8`**: Configuration file for the Flake8 code style checker.
    *   ğŸ“„ **`.gitignore`**: Specifies intentionally untracked files/directories that Git should ignore (e.g., virtual environments `.venv`, log files `logs/`, model artifacts `models/`, local W&B cache `wandb/`, Python cache `__pycache__/`).
    *   ğŸ“„ **`config.yaml`** âš™ï¸: Central configuration file. Defines all crucial paths (data, vocab, embeddings, model saves), hyperparameters for Word2Vec (Week 1) and Two-Tower models (embeddings, architecture, training), and logging settings. Read by `utils/run_utils.py`.
    *   ğŸ“„ **`Dockerfile`**: Instructions for building a Docker container image for the application/service, enabling deployment.
    *   ğŸ“„ **`LICENSE`**: Contains the project's license information (e.g., MIT License).
    *   ğŸ“„ **`marco_entry.txt`**, ğŸ“„ **`ms_marco_entry.txt`**: Appear to be text files containing sample data entries from the MS MARCO dataset, likely used for reference or manual inspection during development.
    *   ğŸ“„ **`PROJECT_DOCUMENTATION.md`**, ğŸ“„ **`PROJECT_DOCUMENTATION.html`**: Auto-generated comprehensive documentation files created by `scripts/generate_project_doc.py`.
    *   ğŸ“„ **`README.md`** ğŸ‘‹: The main entry point for understanding the project. Provides a high-level overview, setup instructions, usage examples, current status, and future work.
    *   ğŸ“„ **`requirements.txt`** ğŸ“¦: Lists all required Python packages and their versions needed to run the project. Used for setting up the environment (`pip install -r requirements.txt`). Includes libraries like `torch`, `datasets`, `wandb`, `PyYAML`, `pandas`, `numpy`, `tqdm`, etc.
    *   ğŸ“„ **`setup_project.py`**: A utility script used initially to create the basic directory structure and template files for the project.

*   ğŸ“ **`app/`**: Contains code for the user-facing application or API service (likely using Streamlit or FastAPI later).
    *   ğŸ“„ **`__init__.py`**: Marks the `app` directory as a Python package.
    *   ğŸ“„ **`app.py`**: The main script for the application (currently contains a dummy Streamlit app). Will eventually integrate the trained model for inference.

*   ğŸ“ **`data/`**: Directory intended for storing input data.
    *   ğŸ“ **`msmarco/`** ğŸ“Š: Designated location for storing the raw MS MARCO v1.1 dataset files downloaded (or cached) by the Hugging Face `datasets` library.

*   ğŸ“ **`data_triplets/`**: *New directory observed.* Appears to be intended for storing pre-generated (Query, Positive Passage, Negative Passage) triplets, potentially as CSV files. This might be used if triplet generation is done offline or if specific pre-defined splits are used.
    *   ğŸ“„ **`test_triplets.csv`**, ğŸ“„ **`train_triplets.csv`**, ğŸ“„ **`val_triplets.csv`**: CSV files likely containing pre-generated query/passage IDs or text for training, validation, and testing using triplets. *(Check if `dataset.py` uses these or generates triplets dynamically)*.

*   ğŸ“ **`docs/`** ğŸ“„: Contains project documentation files.
    *   ğŸ“„ **`DEV_PLAN_W2.md`**: The development plan outlining tasks, assignments, and schedule for Week 2.
    *   ğŸ“„ **`STRUCTURE.MD`**: This file, describing the project structure and file purposes.

*   ğŸ“ **`logs/`** ğŸ“: Stores runtime log files generated by the application (configured in `utils/logging.py`). (Gitignored).

*   ğŸ“ **`models/`** ğŸ§ : Root directory for storing saved model artifacts. (Gitignored).
    *   ğŸ“ **`two_tower/`**: Contains subdirectories specific to trained Two-Tower model runs.
        *   ğŸ“ **`<RUN_NAME>/`** (e.g., `TwoTower_E3_LR1e-05_BS128/`): A subdirectory created for each training run, typically named based on hyperparameters or W&B run name.
            *   ğŸ“„ **`two_tower_final.pth`**: The saved state dictionary (weights) of the trained PyTorch Two-Tower model for that run.
            *   ğŸ“„ **`training_losses.json`**: JSON file storing the list of average training losses per epoch for that run.
            *   ğŸ“„ **`training_loss.png`**: Plot visualizing the training loss per epoch saved as an image.
    *   ğŸ“ **`word2vec/`**: Contains artifacts from the Week 1 Word2Vec training. These are *inputs* for the Week 2 project.
        *   ğŸ“ **`<RUN_NAME>/`** (e.g., `SkipGram_.../`): Subdirectory for a specific Word2Vec training run.
            *   ğŸ“„ **`model_state.pth`**: Saved state dictionary of the *pre-trained Word2Vec embeddings* used by the Two-Tower model.
            *   ğŸ“„ `training_loss.png`, `training_losses.json`: Loss artifacts from the Word2Vec run.
        *   ğŸ“„ **`text8_vocab.json`**: The vocabulary file (mapping words to indices) generated during Word2Vec training, used for tokenizing MS MARCO data.

*   ğŸ“ **`notebooks/`** ğŸ““: Location for Jupyter notebooks used for exploratory data analysis (EDA), model evaluation, visualization, or prototyping.

*   ğŸ“ **`scripts/`** â–¶ï¸: Contains the main executable Python scripts for performing key project tasks.
    *   ğŸ“„ **`evaluate_two_tower.py`**: Script intended for evaluating the trained Two-Tower model using metrics like Recall@k and MRR. (Currently a placeholder).
    *   ğŸ“„ **`generate_project_doc.py`**: Utility script to automatically generate `PROJECT_DOCUMENTATION.md` and `.html` files.
    *   ğŸ“„ **`inspect_hf_dataset.py`**: Utility script for debugging and understanding the structure of the MS MARCO dataset loaded via Hugging Face `datasets`.
    *   ğŸ“„ **`train_two_tower.py`**: The primary script to orchestrate the entire Two-Tower model training process: loads config, loads data/vocab/embeddings, initializes model/optimizer, calls the trainer, saves artifacts, logs to W&B.
    *   ğŸ“„ **`train_word2vec.py`**: The script from Week 1 used to train the Word2Vec model (CBOW or SkipGram) that produces the pre-trained embeddings and vocabulary used in Week 2.

*   ğŸ“ **`src/`** ğŸ: Contains the core source code organized into Python modules.
    *   ğŸ“„ **`__init__.py`**: Makes `src` directory a Python package.
    *   ğŸ“ **`two_tower/`** âœ¨: Modules implementing the Two-Tower search model logic.
        *   ğŸ“„ **`__init__.py`**: Marks `two_tower` as a sub-package.
        *   ğŸ“„ **`dataset.py`**: Contains functions and classes for data handling: `load_msmarco_hf` (loads data), `generate_triplets_from_dataset` (creates Q,P,N triplets), `tokenize_text`, `TripletDataset` (PyTorch Dataset), and `collate_triplets` (batch padding).
        *   ğŸ“„ **`model.py`**: Defines the PyTorch neural network architecture: `QueryEncoder` (RNN-based), `DocumentEncoder` (RNN-based, possibly shared), and the main `TwoTowerModel` class that integrates the embedding layer and the two encoders.
        *   ğŸ“„ **`trainer.py`**: Implements the model training logic: `train_epoch_two_tower` (performs one epoch of training with Triplet Loss) and `train_two_tower_model` (orchestrates the overall training loop over multiple epochs, handles saving, calls epoch trainer).
    *   ğŸ“ **`word2vec/`**: Reused modules from Week 1 related to Word2Vec implementation (Vocabulary, Dataset generation, Model definition, Trainer).
        *   ğŸ“„ `__init__.py`, `dataset.py`, `model.py`, `trainer.py`, `vocabulary.py`
    *   ğŸ“„ **`__init__.py`**: *Duplicate entry in tree? Should likely only be one at `src/` level.*

*   ğŸ“ **`utils/`** ğŸ› ï¸: Contains shared utility modules used across the project.
    *   ğŸ“„ **`__init__.py`**: Marks `utils` as a package and potentially exposes key utility functions.
    *   ğŸ“„ **`device_setup.py`**: Function (`get_device`) to detect and select the appropriate PyTorch compute device (CPU, MPS, CUDA).
    *   ğŸ“„ **`logging.py`**: Configures the project-wide logger (`Perceptron Party`) with specified format, level, and handlers (console, rotating file).
    *   ğŸ“„ **`run_utils.py`**: Helper functions for common tasks like loading the `config.yaml` file, saving/plotting training losses, and formatting numbers.

*   ğŸ“ **`wandb/`** â˜ï¸: Directory created by Weights & Biases to store local run data, logs, and cache before syncing to the cloud. (Gitignored).

