# MS MARCO Search Engine - Configuration
# Copyright (c) 2025 Perceptron Party Team (Yurii, Pyry, Dimitris, Dimitar)
# Created: 2025-04-21
# Updated: 2025-04-21

# --- General Paths ---
paths:
  corpus_file: "data/text8.txt"                    # Path to the training corpus
  vocab_file: "models/word2vec/text8_vocab.json"   # Path to save/load vocabulary
  pretrained_embeddings: "models/word2vec/SkipGram_D128_W5_NWAll_MF5_E3_LR0.001_BS512/model_state.pth"
  two_tower_model_save_dir: "models/two_tower"
  model_save_dir: "models/word2vec"                # Directory to save trained w2v model
  log_dir: "logs"                                  # Directory for log files
  log_file_name: "dropout_disco.log"               # Name for the main log file

# --- Word2Vec Model Hyperparameters ---
word2vec:
  model_type: "SkipGram"              # SkipGram model implementation
  embedding_dim: 128                 # Dimensionality of word embeddings
  window_size: 5                     # Context window size (words on each side)
  min_word_freq: 5                   # Minimum word frequency for vocabulary
  negative_samples: 5                # Number of negative samples for training

# --- Word2Vec Training Hyperparameters ---
word2vec_training:
  num_words_to_process: -1           # Max words from corpus (-1 for all)
  epochs: 15                         # Number of training epochs
  batch_size: 512                    # Training batch size
  learning_rate: 0.001               # Initial learning rate for Adam optimizer
  # Add other training params here if needed (e.g., scheduler settings)

# --- Word Embedding Parameters (from loaded model) ---
embeddings:
  freeze: True # Freeze pre-trained embeddings during tower training?
  # embed_dim will be inferred from loaded file usually

# --- Two Tower Model Hyperparameters ---
two_tower:
  model_type: "RNN" # Could be RNN, GRU, LSTM, BiLSTM etc.
  shared_document_encoder: True # Use same encoder for pos/neg docs?
  hidden_dim: 256 # Dimensionality of RNN hidden state / final encoding
  num_layers: 1 # Number of layers in RNN
  dropout: 0.1 # Dropout probability in RNN
  bidirectional: False # Use bidirectional RNN?

# --- Two Tower Training Hyperparameters ---
two_tower_training:
  epochs: 5
  batch_size: 128
  learning_rate: 0.0005 # Often lower for fine-tuning / triplet loss
  margin: 0.2 # Margin for Triplet Loss
  distance_metric: "cosine" # 'cosine' or 'euclidean'

# --- Logging Configuration ---
logging:
  log_level: "INFO"
  log_file_enabled: True
  log_console_enabled: True
  log_max_bytes: 10485760 # 10 MB
  log_backup_count: 5
